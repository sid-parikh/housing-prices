# -*- coding: utf-8 -*-
"""Housing Price Prediction Fall 2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KXsNkEnCRLgzWyzzKofr4ZXI6Cb8h0pB

# Housing Prices Prediction
"""

import pandas as pd
from joblib import dump, load
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor

bdbiPath = "./data.csv"
bdbiData = pd.read_csv(bdbiPath, index_col='property_id')
print(bdbiData.shape)
bdbiData.head()

y = bdbiData.sold_price

features = ["year_built",
            "sold_date",
            "baths_full",
            "baths_half",
            "lot_sqft",
            "sqft",
            "garage",
            "stories",
            "beds",
            "type",
            "city",
            "Tract",
            "Pct_1Car",
            "Pct_2Car",
            "PopulationDensity",
            "EmploymentDensity",
            "JobsNearby",
            "EmploymentEntropy",
            "Walkability",
            "sold_price"]

X_full = bdbiData[features]
print(X_full.shape)
X_full.head()

"""## Removing rows with empty data
* Dropping NaN values in year_built and sqft
* Substituting NaN values:
  * baths_full, baths_half, garage, : 0
  * stories:
    * if sqft < 2000: 1
    * else: 2
  * beds:
    * if type ==

"""

# Dropping empty values
columns_to_check = ["year_built", "sqft", "type", "city", "sold_price"]

X_full = X_full.dropna(subset=columns_to_check)

X_features =["year_built",
            "sold_date",
            "baths_full",
            "baths_half",
            "lot_sqft",
            "sqft",
            "garage",
            "stories",
            "beds",
            "type",
            "city",
            "Tract",
            "Pct_1Car",
            "Pct_2Car",
            "PopulationDensity",
            "EmploymentDensity",
            "JobsNearby",
            "EmploymentEntropy",
            "Walkability"]
X = X_full[X_features]
y = X_full.sold_price

# Substituting Values (without condition)
replacement_values = {
    'baths_full': 0,
    'baths_half': 0,
    'garage': 0,
    'lot_sqft': 0,  # could find averages of other similar houses and substitute value here
    'beds': 4  # average value of beds
}

X.fillna(value=replacement_values, inplace=True)

# Substituing Values (with condition)
condition = X['sqft'] < 2000.0

X.loc[condition, 'stories'] = X.loc[condition, 'stories'].fillna(1)
X.loc[~condition, 'stories'] = X.loc[~condition, 'stories'].fillna(2)

print(X.shape)

# print number of missing columns
missing_val_count_by_column = (X.isnull().sum())
print("missing values: ", missing_val_count_by_column[missing_val_count_by_column > 0])

print(X)
print(y)

"""## Encoding Data

* Converting dates to floats
* One-Hot encoding Type and City

"""

# Converting dates to floats
X['sold_date'] =  pd.to_datetime(X['sold_date'])
X['sold_year'] = X['sold_date'].dt.year
X['sold_month'] = X['sold_date'].dt.month
X['sold_day'] = X['sold_date'].dt.day
X.drop(columns=['sold_date'], inplace=True)
X

X['type'] = X['type'].replace({'condos': 'condo'})

# One hot encoding
X = pd.get_dummies(X, columns=['type'])
X = pd.get_dummies(X, columns=['city'])
print(X)

"""## Spliting Data into test-train split."""

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

"""## Training Decision Tree Regressor"""

decisionTreeModel= DecisionTreeRegressor(random_state=1)
decisionTreeModel.fit(train_X, train_y)
val_predictions = decisionTreeModel.predict(val_X)
print(val_X.columns.values.tolist())
val_mae = mean_absolute_error(val_predictions, val_y)
print(decisionTreeModel.feature_names_in_)
print("Validation MAE for best value of max_leaf_nodes: {:,.0f}".format(val_mae))
dump(decisionTreeModel, 'model.joblib')


# bounding leaf nodes to 100 just for fun
decisionTreeModelMaxLeaves = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)
decisionTreeModelMaxLeaves.fit(train_X, train_y)
val_predictions = decisionTreeModelMaxLeaves.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE for best value of 100 max leaf nodes: {:,.0f}".format(val_mae))

"""## Training Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor()
rf_model.fit(train_X, train_y)
prediction_val = rf_model.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))


rf_model_dept = RandomForestRegressor(max_depth=100)
rf_model_dept.fit(train_X, train_y)
prediction_val = rf_model_dept.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))

rf_model_est = RandomForestRegressor(n_estimators=100)
rf_model_est.fit(train_X, train_y)
prediction_val = rf_model_est.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))

rf_model_features = RandomForestRegressor(max_features='sqrt', max_depth=100)
rf_model_features.fit(train_X, train_y)
prediction_val = rf_model_features.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))
dump(rf_model_features, 'randomForestModl.joblib')

rf_model_features_log = RandomForestRegressor(max_features='log2')
rf_model_features_log.fit(train_X, train_y)
prediction_val = rf_model_features_log.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))

rf_model_features_const = RandomForestRegressor(max_features=1)
rf_model_features_const.fit(train_X, train_y)
prediction_val = rf_model_features_const.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))

rf_model_features_const = RandomForestRegressor(max_features=1)
rf_model_features_const.fit(train_X, train_y)
prediction_val = rf_model_features_const.predict(val_X)
rf_val_mae = mean_absolute_error(prediction_val, val_y)
print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))